{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internet Archive Science Fiction Downloader\n",
    "\n",
    "Source: https://github.com/jjjake/internetarchive\n",
    "\n",
    "This notebook downloads major science fiction collections from Internet Archive:\n",
    "1. **ultimate-pgsf-txt** - 1,900+ Project Gutenberg SF texts\n",
    "2. **Pulp Magazine Archive** - Thousands of pulp magazine issues (Amazing Stories, Weird Tales, Galaxy, etc.)\n",
    "3. **sciencefiction collection** - Individual SF books and texts (thousands available)\n",
    "\n",
    "Features:\n",
    "- Downloads all files from each collection\n",
    "- Creates metadata.csv for each dataset\n",
    "- Uses checksums to skip already-downloaded files\n",
    "- Extracts: title, author, language, subject, url, local_path, format, size, md5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install internetarchive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from internetarchive import download, get_item, search_items\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure download directory\n",
    "DOWNLOAD_DIR = os.path.expanduser(\"~/scifi_datasets/internet_archive\")\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Download location: {DOWNLOAD_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Definitions\n",
    "\n",
    "### Available Collections:\n",
    "\n",
    "1. **ultimate-pgsf-txt** (1,900+ texts, 87MB)\n",
    "   - Plain text files from Project Gutenberg\n",
    "   - Classic SF authors: Asimov, Leinster, etc.\n",
    "   - Pre-1960s public domain works\n",
    "\n",
    "2. **Pulp Magazine Archive** (Thousands of magazines)\n",
    "   - PDF scans with original layout and artwork\n",
    "   - Amazing Stories (1926+), Weird Tales (1923-1954), Galaxy (355 issues)\n",
    "   - Authors: Asimov, Clarke, Dick, Lovecraft, Bradbury\n",
    "\n",
    "3. **sciencefiction** (Collection with thousands of items)\n",
    "   - Individual books, scholarly works, critical essays\n",
    "   - Mixed content types and time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary collections to download\n",
    "COLLECTIONS = [\n",
    "    {\n",
    "        \"id\": \"ultimate-pgsf-txt\",\n",
    "        \"name\": \"Ultimate Project Gutenberg SF Collection\",\n",
    "        \"description\": \"1,900+ plain text science fiction files from Project Gutenberg\",\n",
    "        \"formats\": [\"Text\"]  # Download only text files\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pulpmagazinearchive\",\n",
    "        \"name\": \"Pulp Magazine Archive (Science Fiction subset)\",\n",
    "        \"description\": \"Thousands of pulp magazine issues - PDF scans with original layout\",\n",
    "        \"formats\": [\"PDF\"],  # Download PDF format\n",
    "        \"query\": \"collection:pulpmagazinearchive AND subject:science fiction\",  # Search query for SF subset\n",
    "        \"use_search\": True  # This needs to be searched, not downloaded directly\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_csv(item, item_dir, csv_path):\n",
    "    \"\"\"\n",
    "    Create a metadata CSV file for an Internet Archive item.\n",
    "    Format: title, author, ia_identifier, language, subject, url, local_path, format, size, md5\n",
    "    \"\"\"\n",
    "    metadata = item.item_metadata.get('metadata', {})\n",
    "    \n",
    "    # Prepare metadata records for each file\n",
    "    records = []\n",
    "    \n",
    "    for file in item.files:\n",
    "        # Skip metadata and derivative files\n",
    "        if file['name'].endswith(('.xml', '.sqlite', '_meta.mrc', '.torrent')):\n",
    "            continue\n",
    "        \n",
    "        # Extract file metadata\n",
    "        file_path = os.path.join(item_dir, file['name'])\n",
    "        \n",
    "        # Get metadata fields\n",
    "        title = metadata.get('title', '')\n",
    "        creator = metadata.get('creator', metadata.get('author', ''))\n",
    "        if isinstance(creator, list):\n",
    "            creator = '; '.join(creator)\n",
    "        \n",
    "        language = metadata.get('language', '')\n",
    "        if isinstance(language, list):\n",
    "            language = '; '.join(language)\n",
    "        \n",
    "        subject = metadata.get('subject', '')\n",
    "        if isinstance(subject, list):\n",
    "            subject = '; '.join(subject)\n",
    "        \n",
    "        identifier = item.identifier\n",
    "        url = f\"https://archive.org/details/{identifier}\"\n",
    "        \n",
    "        records.append({\n",
    "            'title': title,\n",
    "            'author': creator,\n",
    "            'ia_identifier': identifier,\n",
    "            'language': language,\n",
    "            'subject': subject,\n",
    "            'url': url,\n",
    "            'local_path': file_path,\n",
    "            'format': file.get('format', ''),\n",
    "            'size': file.get('size', ''),\n",
    "            'md5': file.get('md5', '')\n",
    "        })\n",
    "    \n",
    "    # Write CSV\n",
    "    if records:\n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            fieldnames = ['title', 'author', 'ia_identifier', 'language', 'subject',\n",
    "                         'url', 'local_path', 'format', 'size', 'md5']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(records)\n",
    "        \n",
    "        print(f\"  ✓ Created metadata CSV: {csv_path}\")\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_collection(collection_info, checksum=True):\n",
    "    \"\"\"\n",
    "    Download a single collection from Internet Archive.\n",
    "    \"\"\"\n",
    "    item_id = collection_info[\"id\"]\n",
    "    name = collection_info[\"name\"]\n",
    "    formats = collection_info.get(\"formats\")\n",
    "    \n",
    "    print(f\"\\n[Downloading] {name}\")\n",
    "    print(f\"Identifier: {item_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Get item metadata first\n",
    "        item = get_item(item_id)\n",
    "        print(f\"Title: {item.item_metadata['metadata'].get('title', 'N/A')}\")\n",
    "        print(f\"Size: {item.item_size / (1024**3):.2f} GB\")\n",
    "        print(f\"Files: {item.files_count}\")\n",
    "        \n",
    "        # Create directory for this collection\n",
    "        item_dir = os.path.join(DOWNLOAD_DIR, item_id)\n",
    "        os.makedirs(item_dir, exist_ok=True)\n",
    "        \n",
    "        # Download with options\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(item_dir)\n",
    "        \n",
    "        if formats:\n",
    "            print(f\"Downloading formats: {', '.join(formats)}\")\n",
    "            download(item_id, verbose=True, checksum=checksum, formats=formats)\n",
    "        else:\n",
    "            print(\"Downloading all formats...\")\n",
    "            download(item_id, verbose=True, checksum=checksum)\n",
    "        \n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        # Create metadata CSV\n",
    "        csv_path = os.path.join(item_dir, 'metadata.csv')\n",
    "        create_metadata_csv(item, item_dir, csv_path)\n",
    "        \n",
    "        print(f\"✓ Successfully downloaded {name}\\n\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {item_id}: {e}\\n\")\n",
    "        os.chdir(original_dir)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_download_collection(query, collection_name, max_items=100, formats=None):\n",
    "    \"\"\"\n",
    "    Search for items matching a query and download them.\n",
    "    Used for collections like Pulp Magazine Archive and sciencefiction.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[Collection Search] {collection_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Downloading up to {max_items} items...\\n\")\n",
    "    \n",
    "    # Create subdirectory for collection items\n",
    "    collection_dir = os.path.join(DOWNLOAD_DIR, collection_name.lower().replace(' ', '_'))\n",
    "    os.makedirs(collection_dir, exist_ok=True)\n",
    "    \n",
    "    # Aggregate metadata CSV for entire collection\n",
    "    aggregate_records = []\n",
    "    \n",
    "    try:\n",
    "        count = 0\n",
    "        for result in search_items(query):\n",
    "            if count >= max_items:\n",
    "                break\n",
    "            \n",
    "            item_id = result['identifier']\n",
    "            print(f\"\\n[{count+1}/{max_items}] Downloading: {item_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Get item metadata\n",
    "                item = get_item(item_id)\n",
    "                \n",
    "                # Create item subdirectory\n",
    "                item_dir = os.path.join(collection_dir, item_id)\n",
    "                os.makedirs(item_dir, exist_ok=True)\n",
    "                \n",
    "                # Download to item subdirectory\n",
    "                original_dir = os.getcwd()\n",
    "                os.chdir(item_dir)\n",
    "                \n",
    "                if formats:\n",
    "                    download(item_id, verbose=True, checksum=True, formats=formats)\n",
    "                else:\n",
    "                    download(item_id, verbose=True, checksum=True)\n",
    "                \n",
    "                os.chdir(original_dir)\n",
    "                \n",
    "                # Create individual metadata CSV\n",
    "                csv_path = os.path.join(item_dir, 'metadata.csv')\n",
    "                if create_metadata_csv(item, item_dir, csv_path):\n",
    "                    # Read records for aggregate CSV\n",
    "                    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "                        reader = csv.DictReader(f)\n",
    "                        aggregate_records.extend(list(reader))\n",
    "                \n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {e}\")\n",
    "                os.chdir(original_dir)\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n✓ Downloaded {count} items from {collection_name}\")\n",
    "        \n",
    "        # Create aggregate metadata CSV\n",
    "        if aggregate_records:\n",
    "            aggregate_csv_path = os.path.join(collection_dir, 'metadata_all.csv')\n",
    "            with open(aggregate_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "                fieldnames = aggregate_records[0].keys()\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(aggregate_records)\n",
    "            print(f\"✓ Created aggregate metadata CSV: {aggregate_csv_path}\")\n",
    "        \n",
    "        return count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error searching collection: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Collections (Optional)\n",
    "\n",
    "Run these cells to preview collection metadata before downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available collections with metadata\n",
    "print(\"Available Science Fiction Collections:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, coll in enumerate(COLLECTIONS, 1):\n",
    "    print(f\"\\n{i}. {coll['name']}\")\n",
    "    print(f\"   ID: {coll['id']}\")\n",
    "    print(f\"   Description: {coll['description']}\")\n",
    "    \n",
    "    # Skip size check for search-based collections\n",
    "    if coll.get('use_search'):\n",
    "        print(f\"   Note: This is a search-based collection with many items\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        item = get_item(coll['id'])\n",
    "        print(f\"   Size: {item.item_size / (1024**3):.2f} GB\")\n",
    "        print(f\"   Files: {item.files_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   (Metadata unavailable: {e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Collections\n",
    "\n",
    "### 1. Download Ultimate Project Gutenberg SF Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Ultimate Project Gutenberg SF Collection\n",
    "download_collection(COLLECTIONS[0], checksum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Pulp Magazine Archive (Science Fiction)\n",
    "\n",
    "⚠️ **Warning**: The Pulp Magazine Archive is HUGE!\n",
    "- Thousands of magazines available\n",
    "- Each magazine is 10-50 MB (PDF scans)\n",
    "- Set a reasonable `max_items` limit below (default: 100)\n",
    "- Start small and increase if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Pulp Magazine Archive - Science Fiction subset\n",
    "# Adjust max_items as needed (100 = ~5GB, 1000 = ~50GB)\n",
    "MAX_PULP_ITEMS = 100\n",
    "\n",
    "pulp_collection = COLLECTIONS[1]\n",
    "search_and_download_collection(\n",
    "    query=pulp_collection['query'],\n",
    "    collection_name=pulp_collection['name'],\n",
    "    max_items=MAX_PULP_ITEMS,\n",
    "    formats=pulp_collection['formats']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (Optional) Download from sciencefiction Collection\n",
    "\n",
    "This collection contains thousands of individual SF books, scholarly works, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download items from the sciencefiction collection\n",
    "# Adjust max_items as needed\n",
    "MAX_SCIFI_ITEMS = 50\n",
    "\n",
    "search_and_download_collection(\n",
    "    query='collection:sciencefiction AND mediatype:texts',\n",
    "    collection_name='Science Fiction Collection',\n",
    "    max_items=MAX_SCIFI_ITEMS,\n",
    "    formats=['Text', 'DjVuTXT']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Specific Collections\n",
    "\n",
    "### Amazing Stories Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Amazing Stories specifically\n",
    "search_and_download_collection(\n",
    "    query='collection:pulpmagazinearchive AND title:\"Amazing Stories\"',\n",
    "    collection_name='Amazing Stories Magazine',\n",
    "    max_items=50,\n",
    "    formats=['PDF']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weird Tales Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Weird Tales specifically\n",
    "search_and_download_collection(\n",
    "    query='collection:pulpmagazinearchive AND title:\"Weird Tales\"',\n",
    "    collection_name='Weird Tales Magazine',\n",
    "    max_items=50,\n",
    "    formats=['PDF']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galaxy Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Galaxy magazine specifically\n",
    "search_and_download_collection(\n",
    "    query='collection:pulpmagazinearchive AND title:\"Galaxy\"',\n",
    "    collection_name='Galaxy Magazine',\n",
    "    max_items=50,\n",
    "    formats=['PDF']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Downloads\n",
    "\n",
    "Check what was downloaded and verify metadata CSVs were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List downloaded collections\n",
    "import os\n",
    "\n",
    "print(\"\\nDownloaded Collections:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for item in os.listdir(DOWNLOAD_DIR):\n",
    "    item_path = os.path.join(DOWNLOAD_DIR, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Count files\n",
    "        files = [f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))]\n",
    "        \n",
    "        # Check for metadata CSV\n",
    "        has_metadata = 'metadata.csv' in files or 'metadata_all.csv' in files\n",
    "        \n",
    "        print(f\"\\n{item}:\")\n",
    "        print(f\"  Files: {len(files)}\")\n",
    "        print(f\"  Has metadata CSV: {'✓' if has_metadata else '✗'}\")\n",
    "        \n",
    "        # Show directory size\n",
    "        total_size = sum(os.path.getsize(os.path.join(item_path, f)) for f in files if os.path.isfile(os.path.join(item_path, f)))\n",
    "        print(f\"  Size: {total_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference: Example NASA Download\n",
    "\n",
    "Original examples from your notebook for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download NASA collection with checksum verification\n",
    "# from internetarchive import download\n",
    "# download('nasa', verbose=True, checksum=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
